## Origins

<aside><p>The inspiration behind the project as told by creator Austin Wright</p></aside>

I think it's important to know the intention of what this project is supposed to be capable of, so here's the background to what I'm trying to accomplish here.

Sometime in early 2009 I was running a Dungeons and Dragons 3.5 game for some other people and I wanted to be able to query various statistics about monsters and the party, all their stats taken into account. For instance, "How much XP does the party get from a Gargantuan Blue Dragon?" or "What are the chances the party will taste good with ketchup (i.e. be defeated by aforementioned Blue Dragon)?" It was shortly after the launch of Wolfwam Alpha, and curious, I tried to see what information it could tell me on this Blue Dragon. It didn't have anything to say on the matter. (Perhaps Stephen Wolfram isn't nerdy enough?) Also around that time, I was playing around with the very cool FRED2 website from the St. Louis Fed that can graph many different sets of economc data, however, I was interested in a particular graph: The price of oil measured by gold (ounces gold per barrel or some similar unit). FRED2, however, does not relate data sets like that, and can't cross reference. Wolfram Alpha choked on the data as well, and at various times either hasn't had the data or doesn't know how to divide two time series (the price of oil in dollars, and the price of an ounce of gold in dollars, dividing the two would result in the price of oil in ounces of gold). I researched what kind of data backend a Wolfram Alpha clone would need, and I quickly found it: RDF, Resource Description Framework. I figured all you need to do is describe the rules of the game, and this Wolfram Alpha clone of sorts would compute the rest. D&D has a lot of data that affects gameplay, so in an effort to find a good way to maintain it, I came accross Semantic Mediawiki, an extension for the Mediawiki (the software written to power Wikipedia) that collects facts from inside webpages and lets you query it, and re-use it within other pages. This did not, however, prove to be an effective way of creating RDF data for external systems to use.

I had also been a big fan of Git, the distributed non-stupid revision control system. Recently I had set up cgit, a Web repository browser for Git written in C (CGIt get it? Har har har nvm), complete with a cache for data and many other features. I began modifying cgit to act as a revisioned wiki of sorts, and began to work on a project that would revision Wikipedia and let you browse it offline, and quickly download updates - almost like the Hitchhiker's Guide. Like Semantic Mediawiki, I intended it to be able to keep an internal database of data that could be queried, transform the documents with XSLT, and even server-side scripting with Javascript. Because of the strong metadata handling I wanted, and to play off "Mediawiki," I called it "metawiki." It didn't get very far, however I could serve static HTML pages somewhat well.

I continued to research RDF and got into programming my own data query tool with PHP, using the ARC library to store and query RDF data in a MySQL database. With this, I was actually able to store data like the oil price, and recall it and spit it out as a graph, with data, modification functions, and UI/graphing functions alike stored in the RDF store - But it was incredibly slow, and I was never able to program a division function of two time series (only a series and a scalar).

Only recently has the actual technology necessary to make this become available: NodeJS, an entirely event-oriented framework designed for high concurrency and efficency. This opened the door to allow persistant database connections; asynchronous, threadless programming; and writing event handlers with the programming language of the Web; among other things.
